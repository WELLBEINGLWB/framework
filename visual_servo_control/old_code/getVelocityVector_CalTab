/***************************************************************************************************
 *
 *   Service for velocity vector from image based visual servo control, using grabbed images
 *   and calculated feature points from a calibration plate in the image.
 *
 **************************************************************************************************/	          
bool getVelocityVector_CalTab(visual_servo_control::request_servo_velocity_vector::Request &req, visual_servo_control::request_servo_velocity_vector::Response &res)
{
	// This part of the function gets locked if it is still calculating the next velocity
	// vector in the servo application. Hence, this service will return false if it is still busy. 
	ROS_INFO("Service called (getVelocityVector_CalTab)");
	std::string request_message = req.request_message.c_str();
	if(!program_lock)
	{
	     ROS_INFO("Program not locked, starting getVelocityVector_CalTab");
		program_lock = true;
          try {
               // If the servo system is not yet initialized, then initialize global variables.
               // The control message is compared to the incoming request message.
               std::string control_message = "initialize";
               if(!servo_initialized && camera_parameters_loaded && request_message == control_message)
               {       
                    ROS_INFO("Initializing servo, camera parameters loaded, request message: %s", control_message.c_str());
                    // We define four 3D points that represent the corners of a 7.5cm by 7.5cm square.              
                    point[0].setWorldCoordinates(-0.375,-0.375, 0);
                    point[1].setWorldCoordinates( 0.375,-0.375, 0);
                    point[2].setWorldCoordinates( 0.375, 0.375, 0);
                    point[3].setWorldCoordinates(-0.375, 0.375, 0);
                    
                    // These are actually the desired features.
                    vpFeatureBuilder::create(pd[0], point[0]);
                    vpFeatureBuilder::create(pd[1], point[1]);
                    vpFeatureBuilder::create(pd[2], point[2]);
                    vpFeatureBuilder::create(pd[3], point[3]);
                  
                    // Set intrinsic camera parameters, since we will have to convert image pixel 
                    // coordinates to visual features expressed in meters.
                    // vpCameraParameters(px,py,u0,v0) where:
                    //   -px = horizontal pixel size. 
                    //   -py = vertical pixel size.
                    //   -u0 = X-Coordinate of image center.
                    //   -v0 = Y-Coordinate of image center.
                    vpCameraParameters camera_params(camera_parameters[7].D(), camera_parameters[8].D(), camera_parameters[9].D(), camera_parameters[10].D());

                    
                    // Output the coordinates as markers for RVIZ to visualize.
                    for (int i = 0 ; i < 4 ; i++) 
                    {
                       visualization_msgs::Marker marker;                  
                       marker.header.frame_id     = "/base";
                       marker.header.stamp        = ros::Time::now();
                       marker.ns                  = ros::this_node::getName().c_str();
                       marker.id                  = i;
                       marker.type                = visualization_msgs::Marker::CYLINDER;
                       marker.action              = visualization_msgs::Marker::ADD;
                       marker.pose.position.x     = point[i].get_oX();
                       marker.pose.position.y     = point[i].get_oY();
                       marker.pose.position.z     = point[i].get_oZ();           
                       marker.pose.orientation.x  = 0.0;
                       marker.pose.orientation.y  = 0.0;
                       marker.pose.orientation.z  = 0.0;
                       marker.pose.orientation.w  = 1.0;
                       marker.scale.x             = 0.05;
                       marker.scale.y             = 0.05;
                       marker.scale.z             = 0.001;
                       marker.color.r             = 1.0f;
                       marker.color.g             = 0.0f;
                       marker.color.b             = 0.0f;
                       marker.color.a             = 1.0;
                       goal_marker_publisher.publish(marker);
                       // A very short sleep is required otherwise only 
                       // a part of the markers are published.
                       ros::Duration(0.001).sleep();
                    }     
                    
                    // The instantiation of the visual servo task is done with the next lines. 
                    // We initialize the task as an eye in hand visual servo. Resulting velocities computed 
                    // by the controller are those that should be applied in the camera frame: Vc 
                    // The interaction matrix will be computed from the current visual features. Thus they 
                    // need to be updated at each iteration of the control loop. Finally, the constant gain 
                    // lambda is set to 0.5. 
                    task.setServo(vpServo::EYEINHAND_CAMERA);
                    task.setInteractionMatrixType(vpServo::CURRENT);
                    task.setLambda(0.5);
                    
                    // It is now time to define four visual features as points in the image-plane. 
                    // To this end we instantiate the vpFeaturePoint class. The current point feature s is 
                    // implemented in p[i]. The desired point feature s* is implemented in pd[i]. 
                    // Each feature is obtained by computing the position of the 3D points in the corresponding 
                    // camera frame, and then by applying the perspective projection. Once current and desired 
                    // features are created, they are added to the visual servo task. 
                  
                    grabRectifiedImage();
                    findPoints();
                    displayGrabbedImage();
                    
                    vpImagePoint feature_1_image_coordinate = vpImagePoint (point_1_x, point_1_y);
                    vpImagePoint feature_2_image_coordinate = vpImagePoint (point_2_x, point_2_y);
                    vpImagePoint feature_3_image_coordinate = vpImagePoint (point_3_x, point_3_y);
                    vpImagePoint feature_4_image_coordinate = vpImagePoint (point_4_x, point_4_y);
                                                        
                                            
                    // Create a vpFeaturePoint using a vpDot and the parameters of the camera. 
                    // The vpDot contains only the pixel coordinates of the point in an image. 
                    // Thus this method uses the camera parameters to compute the meter coordinates 
                    // in x and y in the image plan. Those coordinates are stored in the vpFeaturePoint.
                    vpFeatureBuilder::create(p[0], camera_params, feature_1_image_coordinate);
                    vpFeatureBuilder::create(p[1], camera_params, feature_2_image_coordinate);
                    vpFeatureBuilder::create(p[2], camera_params, feature_3_image_coordinate);
                    vpFeatureBuilder::create(p[3], camera_params, feature_4_image_coordinate);

                    task.addFeature(p[0], pd[0]);
                    task.addFeature(p[1], pd[1]);
                    task.addFeature(p[2], pd[2]);
                    task.addFeature(p[3], pd[3]);

                    // From the initial position wMc of the camera and the position of the object 
                    // previously fixed in the camera frame cMo, we compute the position of the object in the 
                    // world frame wMo. Since in our simulation the object is static, wMo will remain unchanged.
                    
                    // TODO: get position robot/camera from baxter. now hardcoded for testing.
                    //robot.getPosition(wMc);
                    
                    vpHomogeneousMatrix wMc(0, 0, 0.30, 0, 0, 0); 
                    wMo = wMc * cMo;
                    
                    // Update simulated camera pose to starting pose. 
                    vpTranslationVector sim_cam_translation;
                    vpQuaternionVector  sim_cam_rotation;    
                    cMo.extract(sim_cam_translation);
                    cMo.extract(sim_cam_rotation);
                    
                    //ROS_INFO("Quaternion.x: %f", sim_cam_rotation.x());
                    //ROS_INFO("Quaternion.y: %f", sim_cam_rotation.y());
                    //ROS_INFO("Quaternion.z: %f", sim_cam_rotation.z());
                    //ROS_INFO("Quaternion.w: %f", sim_cam_rotation.w());
                   
                    simulated_camera_position = tf::Vector3(sim_cam_translation[0], sim_cam_translation[1], sim_cam_translation[2]);
                    simulated_camera_rotation = tf::Quaternion(sim_cam_rotation.x(), sim_cam_rotation.y(), sim_cam_rotation.z(), sim_cam_rotation.w());

                    // The servo loop will start at iteration zero.
                    iteration = 0;
              			     
                    servo_initialized = true;
                    ROS_INFO("Servo initialized.");
               }  
               else
               {
                    // When initialized, perform servo loop.
                    ROS_INFO("Performing servo loop iteration : %d", iteration);
                    std::string control_message = "cycle";
                    if(simulation_initialized && request_message == control_message)
                    {
                         // Now we can enter in the visual servo loop. When a velocity is applied to our free 
                         // flying camera, the position of the camera frame wMc will evolve wrt the world frame. 
                         // From this position we compute the position of object in the new camera frame.      
                        
                         // Get robot current pose. Should be cast to a vpHomogeneousMatrix.
                         robot.getPosition(wMc);
                         
                         // from the initial position wMc of the camera and the position of the object previously 
                         // fixed in the camera frame cMo, we compute the position of the object in the world 
                         // frame wMo. Since in our simulation the object is static, wMo will remain unchanged.
                         cMo = wMc.inverse() * wMo;
                         
                         // Take images and find feature points.
                         grabRectifiedImage();
                         findPoints();
                         displayGrabbedImage();
                         
                         vpImagePoint feature_1_image_coordinate = vpImagePoint (point_1_x, point_1_y);
                         vpImagePoint feature_2_image_coordinate = vpImagePoint (point_2_x, point_2_y);
                         vpImagePoint feature_3_image_coordinate = vpImagePoint (point_3_x, point_3_y);
                         vpImagePoint feature_4_image_coordinate = vpImagePoint (point_4_x, point_4_y);
                                                 
                         // Set intrinsic camera parameters, since we will have to convert image pixel 
                         // coordinates to visual features expressed in meters.
                         // vpCameraParameters(px,py,u0,v0) where:
                         //   -px = horizontal pixel size. 
                         //   -py = vertical pixel size.
                         //   -u0 = X-Coordinate of image center.
                         //   -v0 = Y-Coordinate of image center.
                         vpCameraParameters camera_params(camera_parameters[7].D(), camera_parameters[8].D(), camera_parameters[9].D(), camera_parameters[10].D());

                                                 
                         // Create a vpFeaturePoint using a vpDot and the parameters of the camera. 
                         // The vpDot contains only the pixel coordinates of the point in an image. 
                         // Thus this method uses the camera parameters to compute the meter coordinates 
                         // in x and y in the image plan. Those coordinates are stored in the vpFeaturePoint.
                         vpFeatureBuilder::create(p[0], camera_params, feature_1_image_coordinate);
                         vpFeatureBuilder::create(p[1], camera_params, feature_2_image_coordinate);
                         vpFeatureBuilder::create(p[2], camera_params, feature_3_image_coordinate);
                         vpFeatureBuilder::create(p[3], camera_params, feature_4_image_coordinate);

                         // It is not possible to compute the depth of the point Z (meters) in the camera frame 
                         // This coordinate is needed in vpFeaturePoint to compute the interaction matrix. 
                         // So this value must be computed / estimated too. 
                         // Furthermore we need to update Z. This is done by projecting each 3D point of the 
                         // target in the camera frame.
                              
                         // TODO: estimateDepthFromImageOrSLAM();
                              
                         vpColVector cP_1;
                         vpColVector cP_2;
                         vpColVector cP_3;
                         vpColVector cP_4;

                         point[0].changeFrame(cMo, cP_1);
                         point[1].changeFrame(cMo, cP_2);
                         point[2].changeFrame(cMo, cP_3);
                         point[3].changeFrame(cMo, cP_4);
                         
                         p[0].set_Z(cP_1[2]);
                         p[1].set_Z(cP_2[2]);
                         p[2].set_Z(cP_3[2]);
                         p[3].set_Z(cP_4[2]);
                         
                         ROS_INFO("Quaternion.w: %f %f %f %f", cP_1[2], cP_2[2], cP_3[2], cP_4[2] );
    
                         // Calculate velocity vector.
                         vpColVector v = task.computeControlLaw();
                         
                         // Output velocity vector.
                         //robot.setVelocity(vpRobot::CAMERA_FRAME, v);
                         
                         
                         // Fill and send velocity vector as response to this service.
                         std::vector<double> velocity_vector(6);
                         velocity_vector[0] = v.operator[](0); 
                         velocity_vector[1] = v.operator[](1); 
                         velocity_vector[2] = v.operator[](2); 
                         velocity_vector[3] = v.operator[](3); 
                         velocity_vector[4] = v.operator[](4); 
                         velocity_vector[5] = v.operator[](5); 
                              
                         // Send response.
                         res.servo_velocity_vector = velocity_vector;
		               ROS_INFO("Velocity vector has been sent: %f %f %f %f %f %f", velocity_vector[0], velocity_vector[1], velocity_vector[2], velocity_vector[3], velocity_vector[4], velocity_vector[5]);

                         // Update simulated camera pose. 
                         vpTranslationVector sim_cam_translation;
                         vpQuaternionVector  sim_cam_rotation;    
                         cMo.extract(sim_cam_translation);
                         cMo.extract(sim_cam_rotation);
                              
                         //ROS_INFO("Quaternion.x: %f", sim_cam_rotation.x());
                         //ROS_INFO("Quaternion.y: %f", sim_cam_rotation.y());
                         //ROS_INFO("Quaternion.z: %f", sim_cam_rotation.z());
                         //ROS_INFO("Quaternion.w: %f", sim_cam_rotation.w());
                             
                         simulated_camera_position = tf::Vector3(sim_cam_translation[0], sim_cam_translation[1], sim_cam_translation[2]);
                         simulated_camera_rotation = tf::Quaternion(sim_cam_rotation.x(), sim_cam_rotation.y(), sim_cam_rotation.z(), sim_cam_rotation.w());
                         
                         iteration++;
                         
                         ROS_INFO("Servo iteration performed, velocity vector send.");
                    }
                    else
                    {
                          if(!servo_initialized)
                          {
                              ROS_INFO("Servo not yet initialized.");
                          }
                          if(request_message != control_message )
                          {
                              ROS_INFO("Wrong request message, use: %s", control_message.c_str() );
                          }
                    }
               }       
          }
          catch(vpException e) 
          {
               ROS_INFO("Could not perform servo control.");
          }     
     	program_lock = false;
		return true;
     }
	else
	{
		ROS_INFO("Visual servo calculation currently already executing and service is therefore locked.");
		return false;
	}
}         			          
          		
